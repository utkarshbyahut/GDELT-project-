#!/usr/bin/env python3
"""
Fetch GDELT articles for each FEMA countyâ€“event record.

Pipeline:
1) Read spine_fema.ndjson (generated by your FEMA script)
2) For each record, build a targeted Doc 2.0 query:
     ("{county}" OR "{fema_designated_area}") AND ("{incidentType}" OR synonyms)
   and time-bound it to [window_start, window_end]
3) Fetch JSON from GDELT Doc 2.0
4) First-pass filter by title; optional LLM rerank hook
5) Write articles.ndjson and articles.csv

Notes:
- Respect GDELT rate limits; sleep between calls.
- Add your own API proxy if needed; Doc 2.0 is GET-only.
"""

import argparse
import csv
import json
import time
import urllib.parse
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import requests

INPUT_NDJSON = "spine_fema.ndjson"
OUT_NDJSON   = "articles.ndjson"
OUT_CSV      = "articles.csv"

GDELT_DOC_API = "https://api.gdeltproject.org/api/v2/doc/doc"
SLEEP_BETWEEN = 0.25      # be nice to the API
RETRIES       = 4
TIMEOUT       = 30
BACKOFF       = 1.6

# Minimal synonyms to widen recall; extend as needed
TYPE_SYNONYMS = {
    "hurricane": ["hurricane", "cyclone", "typhoon", "storm"],
    "storm": ["storm", "severe storm", "thunderstorm", "windstorm"],
    "flood": ["flood", "flash flood", "flooding"],
    "wildfire": ["wildfire", "brush fire", "forest fire"],
    "tornado": ["tornado", "twister"],
    "earthquake": ["earthquake", "tremor"],
    "drought": ["drought"],
}

def safe(s: Optional[str]) -> str:
    return (s or "").strip()

def build_query(county: str, designated: str, incident_type: str) -> str:
    county_q = county.replace('"', '')
    desig_q  = designated.replace('"', '')
    ty = incident_type.lower()
    syns = TYPE_SYNONYMS.get(ty, [ty])
    syn_q = " OR ".join([f"\"{w}\"" for w in syns])
    where_q = " OR ".join([q for q in [f"\"{county_q}\"" if county_q else "", f"\"{desig_q}\"" if desig_q else ""] if q])
    if not where_q:
        where_q = f"\"{county_q or desig_q}\""
    return f"({where_q}) AND ({syn_q})"

def to_gdelt_timerange(iso_date: str) -> str:
    # GDELT expects YYYYMMDDhhmmss; we use midnight
    return iso_date.replace("-", "") + "000000"

def fetch_doc2(params: Dict[str, str]) -> Dict:
    backoff = 1.0
    for _ in range(RETRIES):
        try:
            r = requests.get(GDELT_DOC_API, params=params, timeout=TIMEOUT)
            if r.status_code == 200:
                return r.json()
            if r.status_code in (429, 502, 503, 504):
                time.sleep(backoff)
                backoff *= BACKOFF
                continue
            r.raise_for_status()
        except Exception:
            time.sleep(backoff)
            backoff *= BACKOFF
    return {}

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--spine", type=Path, default=Path(INPUT_NDJSON), help="Input FEMA spine NDJSON")
    ap.add_argument("--out", type=Path, default=Path(OUT_NDJSON), help="Output articles NDJSON")
    ap.add_argument("--csv", type=Path, default=Path(OUT_CSV), help="Output articles CSV")
    ap.add_argument("--per-event-max", type=int, default=30, help="Cap articles per FEMA record")
    ap.add_argument("--min-title-hit", type=int, default=1, help="Require >= N keyword hits in title")
    ap.add_argument("--lang", type=str, default="English", help="Language filter for GDELT")
    ap.add_argument("--query-mode", type=str, default="artlist", help="Doc2 mode: artlist / timelinevol / timelinetone")
    args = ap.parse_args()

    if not args.spine.exists():
        raise SystemExit(f"spine file not found: {args.spine}")

    out_nd = args.out.open("w", encoding="utf-8")
    out_csv = args.csv.open("w", newline="", encoding="utf-8")
    cw = csv.writer(out_csv)
    cw.writerow([
        "disasterNumber","county_fips","state","county","incidentType",
        "window_start","window_end","title","url","datetime","sourceCountry","domain","lang","score","kept_reason"
    ])

    kept_total = 0
    seen_urls = set()
    with args.spine.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            spine = json.loads(line)

            county   = safe(spine.get("county"))
            state    = safe(spine.get("state"))
            desig    = safe(spine.get("fema_designated_area"))
            itype    = safe(spine.get("disaster") or spine.get("fema_incident_type"))
            dnum     = spine.get("fema_disaster_number")
            fips5    = safe(spine.get("county_fips"))
            wstart   = safe(spine.get("window_start"))
            wend     = safe(spine.get("window_end"))
            label    = safe(spine.get("label"))

            if not (wstart and wend and county and state and itype):
                continue

            # Build Doc2 query
            q = build_query(f"{county} County, {state}", desig, itype)
            startdt = to_gdelt_timerange(wstart)
            enddt   = to_gdelt_timerange(wend)

            params = {
                "query": q,
                "mode": args.query_mode,
                "format": "json",
                "startdatetime": startdt,
                "enddatetime": enddt,
                "maxrecords": str(args.per_event_max),
                "sort": "datedesc",
                "timespan": "",      # not used because we give explicit start/end
                "language": args.lang,
            }

            data = fetch_doc2(params)
            articles = data.get("articles", []) or data.get("timeline", []) or []
            if not articles:
                time.sleep(SLEEP_BETWEEN)
                continue

            # First-pass title filter
            hits = []
            # build keyword set from query parts
            kw = set(w.strip("\"").lower() for w in q.replace("(", "").replace(")", "").replace("AND", "").replace("OR", "").split() if len(w.strip("\"")) > 2)
            for a in articles:
                title = a.get("title") or ""
                url   = a.get("url") or a.get("seendateurl") or ""
                if not title or not url:
                    continue
                title_l = title.lower()
                hit_count = sum(1 for k in kw if k in title_l)
                if hit_count >= args.min_title_hit:
                    # Dedup by normalized URL
                    u = url.split("?")[0]
                    if u in seen_urls:
                        continue
                    seen_urls.add(u)
                    rec = {
                        "disasterNumber": dnum,
                        "county_fips": fips5,
                        "state": state,
                        "county": county,
                        "incidentType": itype,
                        "window_start": wstart,
                        "window_end": wend,
                        "title": title,
                        "url": url,
                        "datetime": a.get("seendate") or a.get("date") or "",
                        "sourceCountry": a.get("sourcecountry") or "",
                        "domain": a.get("domain") or "",
                        "lang": a.get("language") or args.lang,
                        "score": hit_count,
                        "kept_reason": f"title_hits>={args.min_title_hit}",
                        "label": label,
                        "gdelt_query": q,
                    }
                    out_nd.write(json.dumps(rec, ensure_ascii=False) + "\n")
                    cw.writerow([
                        dnum, fips5, state, county, itype, wstart, wend,
                        title, url, rec["datetime"], rec["sourceCountry"], rec["domain"], rec["lang"], hit_count, rec["kept_reason"]
                    ])
                    kept_total += 1

            time.sleep(SLEEP_BETWEEN)

    out_nd.close()
    out_csv.close()
    print(f"Done. Wrote {kept_total} articles to {args.out} and {args.csv}")

if __name__ == "__main__":
    main()
